## Теория (кратко и по делу)

*(Подробности постановки и результаты — в отчёте: [docs/report.pdf](docs/report.pdf)).*

### 1) Модель нормальной регрессии

Пусть заданы наблюдения \(y_k\) в узлах \(x_k\). Для полинома степени 2:

$$
y(x)=\theta_0+\theta_1x+\theta_2x^2
$$

Модель наблюдений:

$$
y_k = \theta_0 + \theta_1 x_k + \theta_2 x_k^2 + \varepsilon_k,\quad k=1..n
$$

Ошибки:

$$
\varepsilon_k \sim \mathcal{N}(0,\sigma^2), \quad \varepsilon_1,\dots,\varepsilon_n \text{ независимы}
$$

В матричном виде:

$$
\mathbf{y} = X\theta + \varepsilon,
\quad \varepsilon \sim \mathcal{N}(0,\sigma^2 I_n)
$$

где \(X\) — матрица плана (дизайна), для полинома 2:

$$
X =
\begin{pmatrix}
1 & x_1 & x_1^2\\
\vdots & \vdots & \vdots\\
1 & x_n & x_n^2
\end{pmatrix},
\quad
\theta=
\begin{pmatrix}\theta_0\\\theta_1\\\theta_2\end{pmatrix}
$$

---

### 2) Оценка параметров методом наименьших квадратов (МНК)

МНК-оценка \(\hat\theta\) минимизирует сумму квадратов остатков:

$$
S(\theta) = \lVert \mathbf{y} - X\theta \rVert^2
$$

При условии, что \(X^TX\) обратима:

$$
\hat\theta = (X^TX)^{-1}X^T\mathbf{y}
$$

Остатки:

$$
\hat\varepsilon = \mathbf{y} - X\hat\theta
$$

---

### 3) Распределение МНК-оценки при нормальности ошибок

Если \(\varepsilon \sim \mathcal{N}(0,\sigma^2 I_n)\), то:

$$
\hat\theta \sim \mathcal{N}\left(\theta,\ \sigma^2 (X^TX)^{-1}\right)
$$

То есть ковариационная матрица \(\hat\theta\):

$$
\mathrm{Var}(\hat\theta)=\sigma^2 (X^TX)^{-1}
$$

---

### 4) Оценка дисперсии ошибки

Число параметров: \(p=3\) (для \(\theta_0,\theta_1,\theta_2\)).

Несмещённая оценка дисперсии:

$$
\hat\sigma^2=\frac{\lVert \hat\varepsilon \rVert^2}{n-p}
$$

Важный факт (при нормальности ошибок):

$$
\frac{(n-p)\hat\sigma^2}{\sigma^2}\sim \chi^2_{\,n-p}
$$

---

### 5) Распределение Стьюдента (что это и откуда берётся)

Определение: если

- \(Z \sim \mathcal{N}(0,1)\),
- \(U \sim \chi^2_{\nu}\),
- \(Z\) и \(U\) независимы,

то случайная величина

$$
T=\frac{Z}{\sqrt{U/\nu}}
$$

имеет распределение Стьюдента:

$$
T \sim t_{\nu}
$$

Интуитивно: это “стандартная нормаль, разделённая на случайную оценку стандартного отклонения”.

В регрессии это появляется потому что \(\sigma^2\) неизвестна и заменяется на \(\hat\sigma^2\).

---

### 6) Доверительные интервалы для коэффициентов \(\theta\)

Пусть \(C=(X^TX)^{-1}\). Тогда:

$$
\mathrm{SE}(\hat\theta_i)=\sqrt{\hat\sigma^2 \, C_{ii}}
$$

Центральный \((1-\alpha)\)-интервал для \(\theta_i\):

$$
\hat\theta_i \pm t_{1-\alpha/2,\ n-p}\cdot \sqrt{\hat\sigma^2\,C_{ii}}
$$

где \(t_{1-\alpha/2,\ n-p}\) — квантиль распределения Стьюдента с \(n-p\) степенями свободы.

---

### 7) Доверительный интервал для дисперсии \(\sigma^2\)

Из факта \(\frac{(n-p)\hat\sigma^2}{\sigma^2}\sim \chi^2_{n-p}\) получаем:

$$
\left(
\frac{(n-p)\hat\sigma^2}{\chi^2_{1-\alpha/2,\ n-p}},
\ \frac{(n-p)\hat\sigma^2}{\chi^2_{\alpha/2,\ n-p}}
\right)
$$

---

### 8) Интервалы для полезного сигнала \(y(x)\)

Для точки \(x\) введём вектор регрессоров:

$$
a(x)=\begin{pmatrix}1\\x\\x^2\end{pmatrix}
$$

Оценка сигнала:

$$
\hat y(x)=a(x)^T\hat\theta
$$

Дисперсия оценки среднего (сигнала):

$$
\mathrm{Var}(\hat y(x))=\sigma^2\, a(x)^T (X^TX)^{-1} a(x)
$$

После замены \(\sigma^2\to \hat\sigma^2\):

**Центральный интервал для среднего значения (полезного сигнала):**

$$
\hat y(x) \pm t_{1-\alpha/2,\ n-p}\cdot
\sqrt{\hat\sigma^2\, a(x)^T (X^TX)^{-1} a(x)}
$$

> Если нужен интервал **для нового наблюдения** (а не среднего), то добавляется “+1” под корнем:
> $$
> \hat y(x) \pm t_{1-\alpha/2,\ n-p}\cdot
> \sqrt{\hat\sigma^2\left(1 + a(x)^T (X^TX)^{-1} a(x)\right)}
> $$

---

### 9) Проверка нормальности остатков: \(\chi^2\)-критерий Пирсона

Идея:
1. Строим остатки \(\hat\varepsilon_k\) и стандартизируем (часто используют \(\hat\sigma\)).
2. Делим ось на \(m\) интервалов (корзин).
3. Считаем наблюдённые частоты \(n_i\) в каждом интервале.
4. Считаем ожидаемые вероятности \(p_i\) по нормальному закону и ожидаемые частоты \(np_i\).
5. Считаем статистику:

$$
\chi^2_{\text{набл}}=\sum_{i=1}^{m}\frac{(n_i-np_i)^2}{np_i}
$$

Правило принятия гипотезы \(H_0\) (нормальность) при уровне значимости \(\alpha\):

- либо через критическое значение: \(\chi^2_{\text{набл}} < \chi^2_{1-\alpha,\ \nu}\),
- либо через “двусторонний коридор” (как в выводе программы), если так задано методичкой.

Степени свободы \(\nu\) зависят от того, оценивали ли параметры распределения по данным
(обычно \(\nu = m - 1 - s\), где \(s\) — число оценённых параметров, например \(s=2\) для \(\mu\) и \(\sigma\)).
